{
    __pytorch__: 180
    __v100__: n
    __worker__: 1
    __gpu__: 8
    __cpu__: 20
    __memory__: 40
    __parent__: [
        ${data_dir}/meta
        tests/sample_tasks/checkpoint

        [
            {__def__: {task_name: 'v10.0'}}
        ]

        [
            {learning_rate: 6e-5}
        ]
    ]

    __def__: {
        data_root: oss://xdp-expriment/gaoxing.gx/ydpoc/prepro
        save_root: oss://xdp-expriment/gaoxing.gx/ydpoc/training
    }
    auto_model: google/mt5-base
    pretrained_model: "oss://xdp-expriment/tanfan.zjh/open-dialogue/models/alime/v0.2/checkpoint-125000.pt"

    data_dir: ${data_root}/${task_name}
    save_dir: ${save_root}/${task_name}/${learning_rate}

    save: true
    save_best_only: false
    overwrite: true

    major_metric: loss
    ascending_metric: false
    batch_size: 32
    train_subset: train*
    valid_subset: dev
    lazy_load: true
    update_freq: 16
    max_epoch: 50
    batch_by_len: true

    fp16: true
    gradient_checkpointing: false
    loss: chat
    model: fidt5chat

    optimizer: adam
        adam_eps: 1e-6
        clip_norm: 5.0
        weight_decay: 0.01
        lr_scheduler: one_cycle
        warmup_steps: 100

    eval_interval_warmup: 400
    eval_interval: 500
    log_interval: 20

}
