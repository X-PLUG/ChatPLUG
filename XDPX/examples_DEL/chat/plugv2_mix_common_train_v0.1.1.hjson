{
    __pytorch__: 180
    __v100__: n
    __worker__: 2
    __gpu__: 8
    __cpu__: 20
    __memory__: 40
    __parent__: [
        ${data_dir}/meta
        tests/sample_tasks/checkpoint
        [
            {__def__: {task_name: 'v0.1.1'}}
        ]

        [
            {learning_rate: 1e-4}
        ]
    ]

    __def__: {
        data_root: data_processed
        save_root: data_saved_odps
    }
    pretrained_model: oss://xdp-expriment/zhimiao.chh/pretrain_models/palm2.0/sofa-format/pytorch_model.bin
    data_dir: oss://xdp-expriment/zhimiao.chh/data/dialogue/pretrain/18kw-mix-common/
    save_dir: oss://xdp-expriment/zhimiao.chh/experiment/dialogue/pretrain/18kw_mix_common_sup0.8
    dataset_format: torch

    unsup_task: denoising
    prefix_cut_max_len: 64

    sampler: chat_mix_common
    supervise_ratio: 0.8

    save: true
    auto_suffix: true
    save_full_checkpoint: true
    save_best_only: false
    overwrite: false
    resume: false

    major_metric: loss
    ascending_metric: false
    batch_size: 1024
    train_subset: train*
    valid_subset: dev
    lazy_load: true
    update_freq: 1
    max_epoch: 94
    batch_by_len: false

    processor: chat_mix_common

    fp16: true
    gradient_checkpointing: false
    loss: chat
    model: plugv2_chat
    plug_config_file: xdpx/modules/thirdparty/plugv2/model_configs/palm2.0_chinese_config.json

    optimizer: adam
        adam_eps: 1e-6
        clip_norm: 5.0
        weight_decay: 0.01
        lr_scheduler: one_cycle
        warmup_steps: 50000

    eval_interval_warmup: 5000
    eval_interval: 5000
    log_interval: 50
}
