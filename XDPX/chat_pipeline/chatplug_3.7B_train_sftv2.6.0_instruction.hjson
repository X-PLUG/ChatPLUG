{
    __pytorch__: 180
    __v100__: n
    __worker__: 1
    __gpu__: 8
    __cpu__: 20
    __memory__: 40
    __parent__: [
        ${data_dir}/meta
        [
            {__def__: {task_name: 'v1.1.3'}}
        ]

        [
            {learning_rate: 0.00001}
        ]
    ]

    __def__: {
        data_root: ../data/dialogue/sft/chatplug/belle_instruction
        save_root: ../checkpoints/sft/chatplug/3.7B/v2.6.0_epoch20_lr1e-4_bs512
    }
    
    # auto_model: google/mt5-xl
    auto_model: ../checkpoints/ChatPLUG-3.7B
    pretrained_model: ../checkpoints/ChatPLUG-3.7B/pytorch_model.bin

    data_dir: ${data_root}
    save_dir: ${save_root}

    save: true
    auto_suffix: true
    # save more state like optimizer and lr_schedule, to support resuming training from breakpoint
    save_full_checkpoint: true
    save_best_only: false
    overwrite: false
    resume: false

    major_metric: loss
    ascending_metric: false
    batch_size: 64
    train_subset: train*
    valid_subset: dev
    lazy_load: true
    update_freq: 8
    max_epoch: 20
    batch_by_len: true
	
    deepspeed_save_dir: ${save_root}/ds_states
    # deepspeed_zero_stage==0 means we don't use deepspeed
    deepspeed_zero_stage: 2
    # if your GPU(e.g.A100、V100、T4) support bf16, set deepspeed_bf16: true and deepspeed_fp16: false
    # else set deepspeed_bf16: false and deepspeed_fp16: true
    deepspeed_bf16: true
    deepspeed_fp16: false

    fp16: false
    bf16: false
    gradient_checkpointing: true
    loss: chat
    model: fidt5chat

    optimizer: adam
        adam_eps: 1e-6
        clip_norm: 5.0
        weight_decay: 0.01
        lr_scheduler: one_cycle
        warmup_steps: 4000

    eval_interval_warmup: 50
    eval_interval: 2000
    log_interval: 10

}